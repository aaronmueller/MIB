{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOI Example Submission - DAS Training for Attention Heads\n",
    "\n",
    "This notebook demonstrates how to train DAS (Direct Attribution with Subspace) on attention heads for the IOI (Indirect Object Identification) task using a Gemma model.\n",
    "\n",
    "## Overview\n",
    "1. Load IOI datasets and setup the model\n",
    "2. Learn linear parameters from training data\n",
    "3. Train DAS featurizers on selected attention heads\n",
    "4. Save trained models in submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:13:50.515046Z",
     "iopub.status.busy": "2025-06-28T07:13:50.514869Z",
     "iopub.status.idle": "2025-06-28T07:13:53.964852Z",
     "shell.execute_reply": "2025-06-28T07:13:53.964527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Model: gemma\n",
      "Heads to train: [(7, 6), (8, 1)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve()))\n",
    "\n",
    "from tasks.IOI_task.ioi_task import get_causal_model, get_counterfactual_datasets, get_token_positions\n",
    "from CausalAbstraction.experiments.attention_head_experiment import PatchAttentionHeads\n",
    "from CausalAbstraction.experiments.filter_experiment import FilterExperiment\n",
    "from CausalAbstraction.experiments.aggregate_experiments import attention_head_baselines\n",
    "from baselines.ioi_baselines.ioi_utils import (\n",
    "    log_diff, clear_memory, checker, filter_checker, custom_loss, \n",
    "    ioi_loss_and_metric_fn, setup_pipeline\n",
    ")\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Clear memory before starting\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set device\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"gemma\"  # Will use google/gemma-2-2b\n",
    "heads_list = [(7, 6), (8,1)]  \n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Heads to train: {heads_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data and Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:13:54.005604Z",
     "iopub.status.busy": "2025-06-28T07:13:54.005054Z",
     "iopub.status.idle": "2025-06-28T07:14:02.456854Z",
     "shell.execute_reply": "2025-06-28T07:14:02.456196Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since mib-bench/ioi_private_test couldn't be found on the Hugging Face Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the latest cached dataset configuration 'default' at /home/atticus/.cache/huggingface/datasets/mib-bench___ioi_private_test/default/0.0.0/3513057605230348398b751d59dfeb581c115922 (last modified on Fri May 30 21:55:12 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: dict_keys(['s1_io_flip_train', 's2_io_flip_train', 's1_ioi_flip_s2_ioi_flip_train', 's1_io_flip_test', 's2_io_flip_test', 's1_ioi_flip_s2_ioi_flip_test', 's1_io_flip_testprivate', 's2_io_flip_testprivate', 's1_ioi_flip_s2_ioi_flip_testprivate', 'same_train', 'same_test', 'same_testprivate'])\n",
      "\n",
      "Sample input:\n",
      "  Raw input: As Carl and Maria left the consulate, Carl gave a fridge to\n",
      "  Name A: Carl\n",
      "  Name B: Maria\n",
      "  Name C: Carl\n"
     ]
    }
   ],
   "source": [
    "# Get counterfactual datasets with placeholder causal model\n",
    "# We'll update the causal model with learned parameters later\n",
    "causal_model = get_causal_model({\"bias\": 0.0, \"token_coeff\": 0.0, \"position_coeff\": 0.0})\n",
    "counterfactual_datasets = get_counterfactual_datasets(hf=True, size=1000, load_private_data=True)#None)\n",
    "\n",
    "print(\"Available datasets:\", counterfactual_datasets.keys())\n",
    "\n",
    "# Get a sample to display\n",
    "sample_dataset = next(iter(counterfactual_datasets.values()))\n",
    "if len(sample_dataset) > 0:\n",
    "    sample = sample_dataset[0]\n",
    "    print(\"\\nSample input:\")\n",
    "    print(f\"  Raw input: {sample['input']['raw_input']}\")\n",
    "    print(f\"  Name A: {sample['input']['name_A']}\")\n",
    "    print(f\"  Name B: {sample['input']['name_B']}\")\n",
    "    print(f\"  Name C: {sample['input']['name_C']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:14:02.458396Z",
     "iopub.status.busy": "2025-06-28T07:14:02.458271Z",
     "iopub.status.idle": "2025-06-28T07:14:06.130450Z",
     "shell.execute_reply": "2025-06-28T07:14:06.130083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97191feec80f4bb2a4777008d1affb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have set `use_cache` to `False`, but cache_implementation is set to hybrid. cache_implementation will have no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline device: cuda:1\n",
      "Model: Gemma2ForCausalLM\n",
      "Hidden size: 2304\n",
      "Number of layers: 26\n",
      "\n",
      "Testing model on sample:\n",
      "INPUT: As Carl and Maria left the consulate, Carl gave a fridge to\n",
      "EXPECTED OUTPUT: Maria\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL PREDICTION:  Maria\n"
     ]
    }
   ],
   "source": [
    "# Set up pipeline\n",
    "pipeline, default_batch_size = setup_pipeline(model_name, device, eval_batch_size=None)\n",
    "batch_size = 128  # You can adjust this based on your GPU memory\n",
    "eval_batch_size = 1024\n",
    "\n",
    "print(f\"Pipeline device: {pipeline.model.device}\")\n",
    "print(f\"Model: {pipeline.model.__class__.__name__}\")\n",
    "print(f\"Hidden size: {pipeline.model.config.hidden_size}\")\n",
    "print(f\"Number of layers: {pipeline.get_num_layers()}\")\n",
    "\n",
    "# Test model on a sample\n",
    "if len(sample_dataset) > 0:\n",
    "    sample = sample_dataset[0]\n",
    "    print(\"\\nTesting model on sample:\")\n",
    "    print(f\"INPUT: {sample['input']['raw_input']}\")\n",
    "    expected = causal_model.run_forward(sample['input'])['raw_output']\n",
    "    print(f\"EXPECTED OUTPUT: {expected}\")\n",
    "    print(f\"MODEL PREDICTION: {pipeline.dump(pipeline.generate(sample['input']['raw_input']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Filter Datasets Based on Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:14:06.132422Z",
     "iopub.status.busy": "2025-06-28T07:14:06.132304Z",
     "iopub.status.idle": "2025-06-28T07:15:07.917157Z",
     "shell.execute_reply": "2025-06-28T07:15:07.916760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering datasets based on model performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_io_flip_train:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_io_flip_train: 100%|██████████| 1/1 [00:05<00:00,  5.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_io_flip_train: 100%|██████████| 1/1 [00:05<00:00,  5.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's1_io_flip_train': kept 866/1000 examples (86.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s2_io_flip_train:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s2_io_flip_train: 100%|██████████| 1/1 [00:05<00:00,  5.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s2_io_flip_train: 100%|██████████| 1/1 [00:05<00:00,  5.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's2_io_flip_train': kept 855/1000 examples (85.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_ioi_flip_s2_ioi_flip_train:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_ioi_flip_s2_ioi_flip_train: 100%|██████████| 1/1 [00:05<00:00,  5.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_ioi_flip_s2_ioi_flip_train: 100%|██████████| 1/1 [00:05<00:00,  5.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's1_ioi_flip_s2_ioi_flip_train': kept 864/1000 examples (86.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_io_flip_test:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_io_flip_test: 100%|██████████| 1/1 [00:05<00:00,  5.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_io_flip_test: 100%|██████████| 1/1 [00:05<00:00,  5.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's1_io_flip_test': kept 775/1000 examples (77.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s2_io_flip_test:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s2_io_flip_test: 100%|██████████| 1/1 [00:05<00:00,  5.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s2_io_flip_test: 100%|██████████| 1/1 [00:05<00:00,  5.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's2_io_flip_test': kept 765/1000 examples (76.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_ioi_flip_s2_ioi_flip_test:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_ioi_flip_s2_ioi_flip_test: 100%|██████████| 1/1 [00:05<00:00,  5.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_ioi_flip_s2_ioi_flip_test: 100%|██████████| 1/1 [00:05<00:00,  5.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's1_ioi_flip_s2_ioi_flip_test': kept 772/1000 examples (77.2%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_io_flip_testprivate:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_io_flip_testprivate: 100%|██████████| 1/1 [00:05<00:00,  5.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_io_flip_testprivate: 100%|██████████| 1/1 [00:05<00:00,  5.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's1_io_flip_testprivate': kept 880/1000 examples (88.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s2_io_flip_testprivate:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s2_io_flip_testprivate: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s2_io_flip_testprivate: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's2_io_flip_testprivate': kept 876/1000 examples (87.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_ioi_flip_s2_ioi_flip_testprivate:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_ioi_flip_s2_ioi_flip_testprivate: 100%|██████████| 1/1 [00:05<00:00,  5.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering s1_ioi_flip_s2_ioi_flip_testprivate: 100%|██████████| 1/1 [00:05<00:00,  5.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 's1_ioi_flip_s2_ioi_flip_testprivate': kept 880/1000 examples (88.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering same_train:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering same_train: 100%|██████████| 1/1 [00:05<00:00,  5.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering same_train: 100%|██████████| 1/1 [00:05<00:00,  5.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'same_train': kept 906/1000 examples (90.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering same_test:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering same_test: 100%|██████████| 1/1 [00:05<00:00,  5.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering same_test: 100%|██████████| 1/1 [00:05<00:00,  5.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'same_test': kept 824/1000 examples (82.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering same_testprivate:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering same_testprivate: 100%|██████████| 1/1 [00:05<00:00,  5.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Filtering same_testprivate: 100%|██████████| 1/1 [00:05<00:00,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'same_testprivate': kept 914/1000 examples (91.4%)\n",
      "\n",
      "Total filtering results:\n",
      "Original examples: 12000\n",
      "Kept examples: 10177\n",
      "Overall keep rate: 84.8%\n",
      "\n",
      "Token positions: ['all']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the datasets\n",
    "print(\"Filtering datasets based on model performance...\")\n",
    "exp = FilterExperiment(pipeline, causal_model, filter_checker)\n",
    "filtered_datasets = exp.filter(counterfactual_datasets, verbose=True, batch_size=eval_batch_size)\n",
    "\n",
    "# Get token positions\n",
    "token_positions = get_token_positions(pipeline, causal_model)\n",
    "print(f\"\\nToken positions: {[pos.id for pos in token_positions]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Learn Linear Parameters from Training Data\n",
    "\n",
    "IOI requires linear parameters (bias, token_coeff, position_coeff) for the causal mechanism. We'll learn these from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:15:07.918524Z",
     "iopub.status.busy": "2025-06-28T07:15:07.918401Z",
     "iopub.status.idle": "2025-06-28T07:15:07.922173Z",
     "shell.execute_reply": "2025-06-28T07:15:07.921853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading linear parameters from: baselines/ioi_linear_params.json\n",
      "Using coefficients for gemma:\n",
      "  bias: 0.04835902899503708\n",
      "  token_coeff: 0.767971899360421\n",
      "  position_coeff: 2.004627879709005\n"
     ]
    }
   ],
   "source": [
    "# Load linear parameters from external file\n",
    "linear_params_file = \"baselines/ioi_linear_params.json\"\n",
    "\n",
    "print(f\"Loading linear parameters from: {linear_params_file}\")\n",
    "try:\n",
    "    if os.path.isfile(linear_params_file):\n",
    "        with open(linear_params_file, 'r') as f:\n",
    "            all_coeffs = json.load(f)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Linear parameters file not found: {linear_params_file}\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to load linear_params: {e}\")\n",
    "\n",
    "# Find the coefficients for this model\n",
    "if model_name in all_coeffs:\n",
    "    coeffs = all_coeffs[model_name]\n",
    "elif \"default\" in all_coeffs:\n",
    "    coeffs = all_coeffs[\"default\"]\n",
    "else:\n",
    "    # Use the first available coefficients\n",
    "    coeffs = next(iter(all_coeffs.values()))\n",
    "\n",
    "# Validate required keys\n",
    "required_keys = ['bias', 'token_coeff', 'position_coeff']\n",
    "for key in required_keys:\n",
    "    if key not in coeffs:\n",
    "        raise ValueError(f\"Missing required key '{key}' in linear_coeffs for model {model_name}\")\n",
    "\n",
    "intercept = coeffs['bias']\n",
    "token_coef = coeffs['token_coeff']\n",
    "position_coef = coeffs['position_coeff']\n",
    "\n",
    "print(f\"Using coefficients for {model_name}:\")\n",
    "print(f\"  bias: {intercept}\")\n",
    "print(f\"  token_coeff: {token_coef}\")\n",
    "print(f\"  position_coeff: {position_coef}\")\n",
    "\n",
    "# Store parameters\n",
    "linear_params = {\n",
    "    \"bias\": float(intercept),\n",
    "    \"token_coeff\": float(token_coef),\n",
    "    \"position_coeff\": float(position_coef)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Update Causal Model with Learned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:15:07.923440Z",
     "iopub.status.busy": "2025-06-28T07:15:07.923249Z",
     "iopub.status.idle": "2025-06-28T07:15:08.052180Z",
     "shell.execute_reply": "2025-06-28T07:15:08.051804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal model updated with learned parameters\n"
     ]
    }
   ],
   "source": [
    "# Update the causal model with learned parameters\n",
    "causal_model = get_causal_model(linear_params)\n",
    "print(\"Causal model updated with learned parameters\")\n",
    "\n",
    "# Clear memory before training\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:15:08.053476Z",
     "iopub.status.busy": "2025-06-28T07:15:08.053356Z",
     "iopub.status.idle": "2025-06-28T07:15:08.056697Z",
     "shell.execute_reply": "2025-06-28T07:15:08.056402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datasets: ['s1_io_flip_train', 's2_io_flip_train', 's1_ioi_flip_s2_ioi_flip_train']\n",
      "Test datasets: ['s1_io_flip_test', 's1_io_flip_testprivate', 's2_io_flip_test', 's2_io_flip_testprivate', 's1_ioi_flip_s2_ioi_flip_test', 's1_ioi_flip_s2_ioi_flip_testprivate']\n"
     ]
    }
   ],
   "source": [
    "# Setup counterfactual names for IOI\n",
    "counterfactuals = [\"s1_io_flip\", \"s2_io_flip\", \"s1_ioi_flip_s2_ioi_flip\"]\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for counterfactual in counterfactuals:\n",
    "    if \"same\" in counterfactual:\n",
    "        print(\"hi\")\n",
    "        continue\n",
    "    if counterfactual + \"_train\" in filtered_datasets:\n",
    "        train_data[counterfactual + \"_train\"] = filtered_datasets[counterfactual + \"_train\"]  # Limit to 1000 samples for training\n",
    "    if counterfactual + \"_test\" in filtered_datasets:\n",
    "        test_data[counterfactual + \"_test\"] = filtered_datasets[counterfactual + \"_test\"]\n",
    "    if counterfactual + \"_testprivate\" in filtered_datasets:\n",
    "        test_data[counterfactual + \"_testprivate\"] = filtered_datasets[counterfactual + \"_testprivate\"]\n",
    "\n",
    "print(\"Train datasets:\", list(train_data.keys()))\n",
    "print(\"Test datasets:\", list(test_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:15:08.057872Z",
     "iopub.status.busy": "2025-06-28T07:15:08.057756Z",
     "iopub.status.idle": "2025-06-28T07:15:08.061084Z",
     "shell.execute_reply": "2025-06-28T07:15:08.060797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: ioi_submission_results\n",
      "Models will be saved to: ioi_submission\n"
     ]
    }
   ],
   "source": [
    "# Setup experiment configuration for DAS\n",
    "config = {\n",
    "    \"evaluation_batch_size\": eval_batch_size,\n",
    "    \"batch_size\": batch_size, \n",
    "    \"training_epoch\": 2,  # Number of training epochs\n",
    "    \"check_raw\": True,\n",
    "    \"n_features\": 32,  # Feature dimension for DAS\n",
    "    \"regularization_coefficient\": 0.0, \n",
    "    \"output_scores\": True, \n",
    "    \"shuffle\": True, \n",
    "    \"temperature_schedule\": (1.0, 0.01),  # Temperature annealing for training\n",
    "    \"init_lr\": 1.0,\n",
    "    \"loss_and_metric_fn\": lambda pipeline, intervenable_model, batch, model_units_list: \n",
    "        ioi_loss_and_metric_fn(pipeline, intervenable_model, batch, model_units_list),\n",
    "}\n",
    "\n",
    "# Setup directories for saving results\n",
    "results_dir = \"ioi_submission_results\"\n",
    "model_dir = \"ioi_submission\"\n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "print(f\"Results will be saved to: {results_dir}\")\n",
    "print(f\"Models will be saved to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train DAS on Output Position Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:15:08.062416Z",
     "iopub.status.busy": "2025-06-28T07:15:08.062121Z",
     "iopub.status.idle": "2025-06-28T07:16:34.540689Z",
     "shell.execute_reply": "2025-06-28T07:16:34.540012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training DAS for output_position variable...\n",
      "============================================================\n",
      "Running DAS method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:   0%|          | 0/21 [00:00<?, ?it/s, loss=165, mse=165, rmse=12.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:   5%|▍         | 1/21 [00:01<00:24,  1.23s/it, loss=165, mse=165, rmse=12.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:   5%|▍         | 1/21 [00:01<00:24,  1.23s/it, loss=161, mse=156, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  10%|▉         | 2/21 [00:02<00:22,  1.19s/it, loss=161, mse=156, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  10%|▉         | 2/21 [00:03<00:22,  1.19s/it, loss=160, mse=159, rmse=12.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  14%|█▍        | 3/21 [00:03<00:21,  1.17s/it, loss=160, mse=159, rmse=12.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  14%|█▍        | 3/21 [00:04<00:21,  1.17s/it, loss=161, mse=163, rmse=12.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  19%|█▉        | 4/21 [00:04<00:19,  1.17s/it, loss=161, mse=163, rmse=12.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  19%|█▉        | 4/21 [00:05<00:19,  1.17s/it, loss=160, mse=157, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  24%|██▍       | 5/21 [00:05<00:18,  1.16s/it, loss=160, mse=157, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  24%|██▍       | 5/21 [00:06<00:18,  1.16s/it, loss=159, mse=152, rmse=12.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  29%|██▊       | 6/21 [00:07<00:17,  1.16s/it, loss=159, mse=152, rmse=12.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  29%|██▊       | 6/21 [00:07<00:17,  1.16s/it, loss=159, mse=159, rmse=12.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  33%|███▎      | 7/21 [00:08<00:16,  1.16s/it, loss=159, mse=159, rmse=12.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  33%|███▎      | 7/21 [00:08<00:16,  1.16s/it, loss=157, mse=144, rmse=12]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  38%|███▊      | 8/21 [00:09<00:15,  1.16s/it, loss=157, mse=144, rmse=12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  38%|███▊      | 8/21 [00:10<00:15,  1.16s/it, loss=157, mse=155, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  43%|████▎     | 9/21 [00:10<00:13,  1.16s/it, loss=157, mse=155, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  43%|████▎     | 9/21 [00:11<00:13,  1.16s/it, loss=156, mse=152, rmse=12.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  48%|████▊     | 10/21 [00:11<00:12,  1.15s/it, loss=156, mse=152, rmse=12.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  48%|████▊     | 10/21 [00:12<00:12,  1.15s/it, loss=156, mse=152, rmse=12.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  52%|█████▏    | 11/21 [00:12<00:11,  1.16s/it, loss=156, mse=152, rmse=12.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  52%|█████▏    | 11/21 [00:13<00:11,  1.16s/it, loss=155, mse=146, rmse=12.1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  57%|█████▋    | 12/21 [00:13<00:10,  1.16s/it, loss=155, mse=146, rmse=12.1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  57%|█████▋    | 12/21 [00:14<00:10,  1.16s/it, loss=155, mse=156, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  62%|██████▏   | 13/21 [00:15<00:09,  1.16s/it, loss=155, mse=156, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  62%|██████▏   | 13/21 [00:15<00:09,  1.16s/it, loss=155, mse=150, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  67%|██████▋   | 14/21 [00:16<00:08,  1.16s/it, loss=155, mse=150, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  67%|██████▋   | 14/21 [00:16<00:08,  1.16s/it, loss=155, mse=164, rmse=12.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  71%|███████▏  | 15/21 [00:17<00:06,  1.16s/it, loss=155, mse=164, rmse=12.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  71%|███████▏  | 15/21 [00:18<00:06,  1.16s/it, loss=156, mse=163, rmse=12.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  76%|███████▌  | 16/21 [00:18<00:05,  1.16s/it, loss=156, mse=163, rmse=12.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  76%|███████▌  | 16/21 [00:19<00:05,  1.16s/it, loss=155, mse=145, rmse=12.1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  81%|████████  | 17/21 [00:19<00:04,  1.16s/it, loss=155, mse=145, rmse=12.1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  81%|████████  | 17/21 [00:20<00:04,  1.16s/it, loss=155, mse=154, rmse=12.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  86%|████████▌ | 18/21 [00:20<00:03,  1.16s/it, loss=155, mse=154, rmse=12.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  86%|████████▌ | 18/21 [00:21<00:03,  1.16s/it, loss=155, mse=154, rmse=12.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  90%|█████████ | 19/21 [00:22<00:02,  1.16s/it, loss=155, mse=154, rmse=12.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  90%|█████████ | 19/21 [00:22<00:02,  1.16s/it, loss=155, mse=155, rmse=12.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  95%|█████████▌| 20/21 [00:23<00:01,  1.16s/it, loss=155, mse=155, rmse=12.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  95%|█████████▌| 20/21 [00:23<00:01,  1.16s/it, loss=155, mse=160, rmse=12.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0: 100%|██████████| 21/21 [00:23<00:00,  1.13it/s, loss=155, mse=160, rmse=12.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0: 100%|██████████| 21/21 [00:23<00:00,  1.12s/it, loss=155, mse=160, rmse=12.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "Epoch:  50%|█████     | 1/2 [00:23<00:23, 23.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:   0%|          | 0/21 [00:00<?, ?it/s, loss=156, mse=156, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:   5%|▍         | 1/21 [00:01<00:25,  1.28s/it, loss=156, mse=156, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:   5%|▍         | 1/21 [00:02<00:25,  1.28s/it, loss=151, mse=145, rmse=12]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  10%|▉         | 2/21 [00:02<00:23,  1.22s/it, loss=151, mse=145, rmse=12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  10%|▉         | 2/21 [00:03<00:23,  1.22s/it, loss=151, mse=152, rmse=12.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  14%|█▍        | 3/21 [00:03<00:21,  1.19s/it, loss=151, mse=152, rmse=12.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  14%|█▍        | 3/21 [00:04<00:21,  1.19s/it, loss=149, mse=145, rmse=12.1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  19%|█▉        | 4/21 [00:04<00:20,  1.18s/it, loss=149, mse=145, rmse=12.1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  19%|█▉        | 4/21 [00:05<00:20,  1.18s/it, loss=149, mse=146, rmse=12.1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  24%|██▍       | 5/21 [00:05<00:18,  1.17s/it, loss=149, mse=146, rmse=12.1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  24%|██▍       | 5/21 [00:06<00:18,  1.17s/it, loss=149, mse=149, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  29%|██▊       | 6/21 [00:07<00:17,  1.17s/it, loss=149, mse=149, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  29%|██▊       | 6/21 [00:07<00:17,  1.17s/it, loss=150, mse=155, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  33%|███▎      | 7/21 [00:08<00:16,  1.16s/it, loss=150, mse=155, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  33%|███▎      | 7/21 [00:08<00:16,  1.16s/it, loss=150, mse=156, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  38%|███▊      | 8/21 [00:09<00:15,  1.16s/it, loss=150, mse=156, rmse=12.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  38%|███▊      | 8/21 [00:10<00:15,  1.16s/it, loss=151, mse=158, rmse=12.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  43%|████▎     | 9/21 [00:10<00:13,  1.16s/it, loss=151, mse=158, rmse=12.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  43%|████▎     | 9/21 [00:11<00:13,  1.16s/it, loss=152, mse=153, rmse=12.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  48%|████▊     | 10/21 [00:11<00:12,  1.16s/it, loss=152, mse=153, rmse=12.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  48%|████▊     | 10/21 [00:12<00:12,  1.16s/it, loss=151, mse=149, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  52%|█████▏    | 11/21 [00:12<00:11,  1.16s/it, loss=151, mse=149, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  52%|█████▏    | 11/21 [00:13<00:11,  1.16s/it, loss=150, mse=141, rmse=11.9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  57%|█████▋    | 12/21 [00:14<00:10,  1.16s/it, loss=150, mse=141, rmse=11.9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  57%|█████▋    | 12/21 [00:14<00:10,  1.16s/it, loss=152, mse=165, rmse=12.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  62%|██████▏   | 13/21 [00:15<00:09,  1.16s/it, loss=152, mse=165, rmse=12.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  62%|██████▏   | 13/21 [00:15<00:09,  1.16s/it, loss=152, mse=151, rmse=12.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  67%|██████▋   | 14/21 [00:16<00:08,  1.16s/it, loss=152, mse=151, rmse=12.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  67%|██████▋   | 14/21 [00:17<00:08,  1.16s/it, loss=151, mse=150, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  71%|███████▏  | 15/21 [00:17<00:06,  1.16s/it, loss=151, mse=150, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  71%|███████▏  | 15/21 [00:18<00:06,  1.16s/it, loss=152, mse=162, rmse=12.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  76%|███████▌  | 16/21 [00:18<00:05,  1.16s/it, loss=152, mse=162, rmse=12.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  76%|███████▌  | 16/21 [00:19<00:05,  1.16s/it, loss=152, mse=150, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  81%|████████  | 17/21 [00:19<00:04,  1.16s/it, loss=152, mse=150, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  81%|████████  | 17/21 [00:20<00:04,  1.16s/it, loss=152, mse=149, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  86%|████████▌ | 18/21 [00:20<00:03,  1.16s/it, loss=152, mse=149, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  86%|████████▌ | 18/21 [00:21<00:03,  1.16s/it, loss=152, mse=152, rmse=12.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  90%|█████████ | 19/21 [00:22<00:02,  1.16s/it, loss=152, mse=152, rmse=12.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  90%|█████████ | 19/21 [00:22<00:02,  1.16s/it, loss=152, mse=150, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  95%|█████████▌| 20/21 [00:23<00:01,  1.16s/it, loss=152, mse=150, rmse=12.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  95%|█████████▌| 20/21 [00:23<00:01,  1.16s/it, loss=151, mse=136, rmse=11.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1: 100%|██████████| 21/21 [00:23<00:00,  1.13it/s, loss=151, mse=136, rmse=11.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1: 100%|██████████| 21/21 [00:23<00:00,  1.12s/it, loss=151, mse=136, rmse=11.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "Epoch: 100%|██████████| 2/2 [00:47<00:00, 23.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 100%|██████████| 2/2 [00:47<00:00, 23.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DAS training completed for output_position\n",
      "Models saved to: ioi_submission/ioi_task_Gemma2ForCausalLM_output_position\n"
     ]
    }
   ],
   "source": [
    "# Train DAS for output_position variable\n",
    "print(\"\\nTraining DAS for output_position variable...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fix for in-place operation error during training\n",
    "# Set model to training mode and disable in-place operations\n",
    "pipeline.model.train()\n",
    "if hasattr(pipeline.model.config, 'use_cache'):\n",
    "    pipeline.model.config.use_cache = False\n",
    "\n",
    "target_variable = \"output_position\"\n",
    "position_model_dir = os.path.join(model_dir, f\"ioi_task_{pipeline.model.__class__.__name__}_{target_variable}\")\n",
    "\n",
    "attention_head_baselines(\n",
    "    pipeline=pipeline, \n",
    "    task=causal_model, \n",
    "    token_positions=token_positions, \n",
    "    train_data=train_data, \n",
    "    test_data=test_data, \n",
    "    config=config, \n",
    "    target_variables=[target_variable], \n",
    "    checker=lambda logits, params: checker(logits, params, pipeline), \n",
    "    verbose=True, \n",
    "    results_dir=results_dir,\n",
    "    model_dir=position_model_dir,\n",
    "    heads_list=heads_list,\n",
    "    skip=[\"full_vector\", \"DBM+SVD\", \"DBM+PCA\", \"DBM\", \"DBM+SAE\"]  # Only run DAS\n",
    ")\n",
    "\n",
    "print(f\"\\nDAS training completed for {target_variable}\")\n",
    "print(f\"Models saved to: {position_model_dir}\")\n",
    "\n",
    "# Clear memory\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train DAS on Output Token Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:16:34.542099Z",
     "iopub.status.busy": "2025-06-28T07:16:34.541967Z",
     "iopub.status.idle": "2025-06-28T07:18:00.653167Z",
     "shell.execute_reply": "2025-06-28T07:18:00.652798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training DAS for output_token variable...\n",
      "============================================================\n",
      "Running DAS method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:   0%|          | 0/21 [00:00<?, ?it/s, loss=133, mse=133, rmse=11.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:   5%|▍         | 1/21 [00:01<00:26,  1.34s/it, loss=133, mse=133, rmse=11.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:   5%|▍         | 1/21 [00:02<00:26,  1.34s/it, loss=128, mse=122, rmse=11.1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  10%|▉         | 2/21 [00:02<00:23,  1.24s/it, loss=128, mse=122, rmse=11.1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  10%|▉         | 2/21 [00:03<00:23,  1.24s/it, loss=124, mse=116, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  14%|█▍        | 3/21 [00:03<00:21,  1.20s/it, loss=124, mse=116, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  14%|█▍        | 3/21 [00:04<00:21,  1.20s/it, loss=122, mse=116, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  19%|█▉        | 4/21 [00:04<00:20,  1.18s/it, loss=122, mse=116, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  19%|█▉        | 4/21 [00:05<00:20,  1.18s/it, loss=119, mse=109, rmse=10.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  24%|██▍       | 5/21 [00:05<00:18,  1.17s/it, loss=119, mse=109, rmse=10.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  24%|██▍       | 5/21 [00:06<00:18,  1.17s/it, loss=117, mse=109, rmse=10.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  29%|██▊       | 6/21 [00:07<00:17,  1.16s/it, loss=117, mse=109, rmse=10.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  29%|██▊       | 6/21 [00:07<00:17,  1.16s/it, loss=116, mse=106, rmse=10.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  33%|███▎      | 7/21 [00:08<00:16,  1.16s/it, loss=116, mse=106, rmse=10.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  33%|███▎      | 7/21 [00:08<00:16,  1.16s/it, loss=116, mse=117, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  38%|███▊      | 8/21 [00:09<00:15,  1.16s/it, loss=116, mse=117, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  38%|███▊      | 8/21 [00:10<00:15,  1.16s/it, loss=115, mse=110, rmse=10.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  43%|████▎     | 9/21 [00:10<00:13,  1.15s/it, loss=115, mse=110, rmse=10.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  43%|████▎     | 9/21 [00:11<00:13,  1.15s/it, loss=115, mse=113, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  48%|████▊     | 10/21 [00:11<00:12,  1.16s/it, loss=115, mse=113, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  48%|████▊     | 10/21 [00:12<00:12,  1.16s/it, loss=115, mse=114, rmse=10.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  52%|█████▏    | 11/21 [00:12<00:11,  1.16s/it, loss=115, mse=114, rmse=10.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  52%|█████▏    | 11/21 [00:13<00:11,  1.16s/it, loss=115, mse=116, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  57%|█████▋    | 12/21 [00:14<00:10,  1.16s/it, loss=115, mse=116, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  57%|█████▋    | 12/21 [00:14<00:10,  1.16s/it, loss=116, mse=121, rmse=11]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  62%|██████▏   | 13/21 [00:15<00:09,  1.15s/it, loss=116, mse=121, rmse=11]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  62%|██████▏   | 13/21 [00:15<00:09,  1.15s/it, loss=115, mse=114, rmse=10.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  67%|██████▋   | 14/21 [00:16<00:08,  1.15s/it, loss=115, mse=114, rmse=10.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  67%|██████▋   | 14/21 [00:17<00:08,  1.15s/it, loss=116, mse=120, rmse=11]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  71%|███████▏  | 15/21 [00:17<00:06,  1.15s/it, loss=116, mse=120, rmse=11]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  71%|███████▏  | 15/21 [00:18<00:06,  1.15s/it, loss=115, mse=108, rmse=10.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  76%|███████▌  | 16/21 [00:18<00:05,  1.15s/it, loss=115, mse=108, rmse=10.4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  76%|███████▌  | 16/21 [00:19<00:05,  1.15s/it, loss=115, mse=117, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  81%|████████  | 17/21 [00:19<00:04,  1.15s/it, loss=115, mse=117, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  81%|████████  | 17/21 [00:20<00:04,  1.15s/it, loss=115, mse=111, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  86%|████████▌ | 18/21 [00:20<00:03,  1.15s/it, loss=115, mse=111, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  86%|████████▌ | 18/21 [00:21<00:03,  1.15s/it, loss=115, mse=110, rmse=10.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  90%|█████████ | 19/21 [00:22<00:02,  1.15s/it, loss=115, mse=110, rmse=10.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  90%|█████████ | 19/21 [00:22<00:02,  1.15s/it, loss=115, mse=127, rmse=11.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  95%|█████████▌| 20/21 [00:23<00:01,  1.15s/it, loss=115, mse=127, rmse=11.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0:  95%|█████████▌| 20/21 [00:23<00:01,  1.15s/it, loss=116, mse=125, rmse=11.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0: 100%|██████████| 21/21 [00:23<00:00,  1.13it/s, loss=116, mse=125, rmse=11.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 0: 100%|██████████| 21/21 [00:23<00:00,  1.12s/it, loss=116, mse=125, rmse=11.2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "Epoch:  50%|█████     | 1/2 [00:23<00:23, 23.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:   0%|          | 0/21 [00:00<?, ?it/s, loss=110, mse=110, rmse=10.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:   5%|▍         | 1/21 [00:01<00:23,  1.16s/it, loss=110, mse=110, rmse=10.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:   5%|▍         | 1/21 [00:01<00:23,  1.16s/it, loss=112, mse=115, rmse=10.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  10%|▉         | 2/21 [00:02<00:22,  1.16s/it, loss=112, mse=115, rmse=10.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  10%|▉         | 2/21 [00:03<00:22,  1.16s/it, loss=112, mse=112, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  14%|█▍        | 3/21 [00:03<00:20,  1.16s/it, loss=112, mse=112, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  14%|█▍        | 3/21 [00:04<00:20,  1.16s/it, loss=112, mse=112, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  19%|█▉        | 4/21 [00:04<00:19,  1.16s/it, loss=112, mse=112, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  19%|█▉        | 4/21 [00:05<00:19,  1.16s/it, loss=112, mse=113, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  24%|██▍       | 5/21 [00:05<00:18,  1.15s/it, loss=112, mse=113, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  24%|██▍       | 5/21 [00:06<00:18,  1.15s/it, loss=112, mse=112, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  29%|██▊       | 6/21 [00:06<00:17,  1.15s/it, loss=112, mse=112, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  29%|██▊       | 6/21 [00:07<00:17,  1.15s/it, loss=112, mse=111, rmse=10.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  33%|███▎      | 7/21 [00:08<00:16,  1.16s/it, loss=112, mse=111, rmse=10.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  33%|███▎      | 7/21 [00:08<00:16,  1.16s/it, loss=112, mse=114, rmse=10.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  38%|███▊      | 8/21 [00:09<00:15,  1.16s/it, loss=112, mse=114, rmse=10.7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  38%|███▊      | 8/21 [00:09<00:15,  1.16s/it, loss=113, mse=119, rmse=10.9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  43%|████▎     | 9/21 [00:10<00:13,  1.16s/it, loss=113, mse=119, rmse=10.9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  43%|████▎     | 9/21 [00:11<00:13,  1.16s/it, loss=114, mse=123, rmse=11.1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  48%|████▊     | 10/21 [00:11<00:12,  1.16s/it, loss=114, mse=123, rmse=11.1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  48%|████▊     | 10/21 [00:12<00:12,  1.16s/it, loss=114, mse=116, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  52%|█████▏    | 11/21 [00:12<00:11,  1.16s/it, loss=114, mse=116, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  52%|█████▏    | 11/21 [00:13<00:11,  1.16s/it, loss=113, mse=105, rmse=10.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  57%|█████▋    | 12/21 [00:13<00:10,  1.16s/it, loss=113, mse=105, rmse=10.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  57%|█████▋    | 12/21 [00:14<00:10,  1.16s/it, loss=113, mse=107, rmse=10.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  62%|██████▏   | 13/21 [00:15<00:09,  1.16s/it, loss=113, mse=107, rmse=10.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  62%|██████▏   | 13/21 [00:15<00:09,  1.16s/it, loss=113, mse=113, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  67%|██████▋   | 14/21 [00:16<00:08,  1.16s/it, loss=113, mse=113, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  67%|██████▋   | 14/21 [00:16<00:08,  1.16s/it, loss=113, mse=119, rmse=10.9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  71%|███████▏  | 15/21 [00:17<00:06,  1.15s/it, loss=113, mse=119, rmse=10.9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  71%|███████▏  | 15/21 [00:18<00:06,  1.15s/it, loss=113, mse=107, rmse=10.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  76%|███████▌  | 16/21 [00:18<00:05,  1.16s/it, loss=113, mse=107, rmse=10.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  76%|███████▌  | 16/21 [00:19<00:05,  1.16s/it, loss=113, mse=106, rmse=10.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  81%|████████  | 17/21 [00:19<00:04,  1.16s/it, loss=113, mse=106, rmse=10.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  81%|████████  | 17/21 [00:20<00:04,  1.16s/it, loss=112, mse=111, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  86%|████████▌ | 18/21 [00:20<00:03,  1.16s/it, loss=112, mse=111, rmse=10.6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  86%|████████▌ | 18/21 [00:21<00:03,  1.16s/it, loss=112, mse=111, rmse=10.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  90%|█████████ | 19/21 [00:21<00:02,  1.16s/it, loss=112, mse=111, rmse=10.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  90%|█████████ | 19/21 [00:22<00:02,  1.16s/it, loss=113, mse=115, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  95%|█████████▌| 20/21 [00:23<00:01,  1.16s/it, loss=113, mse=115, rmse=10.8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1:  95%|█████████▌| 20/21 [00:23<00:01,  1.16s/it, loss=113, mse=127, rmse=11.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1: 100%|██████████| 21/21 [00:23<00:00,  1.13it/s, loss=113, mse=127, rmse=11.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 1: 100%|██████████| 21/21 [00:23<00:00,  1.11s/it, loss=113, mse=127, rmse=11.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "Epoch: 100%|██████████| 2/2 [00:46<00:00, 23.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 100%|██████████| 2/2 [00:46<00:00, 23.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DAS training completed for output_token\n",
      "Models saved to: ioi_submission/ioi_task_Gemma2ForCausalLM_output_token\n"
     ]
    }
   ],
   "source": [
    "# Train DAS for output_token variable\n",
    "print(\"\\nTraining DAS for output_token variable...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "target_variable = \"output_token\"\n",
    "token_model_dir = os.path.join(model_dir, f\"ioi_task_{pipeline.model.__class__.__name__}_{target_variable}\")\n",
    "\n",
    "config[\"n_features\"] = 32\n",
    "\n",
    "attention_head_baselines(\n",
    "    pipeline=pipeline, \n",
    "    task=causal_model, \n",
    "    token_positions=token_positions, \n",
    "    train_data=train_data, \n",
    "    test_data=test_data, \n",
    "    config=config, \n",
    "    target_variables=[target_variable], \n",
    "    checker=lambda logits, params: checker(logits, params, pipeline), \n",
    "    verbose=True, \n",
    "    results_dir=results_dir,\n",
    "    model_dir=token_model_dir,\n",
    "    heads_list=heads_list,\n",
    "    skip=[\"full_vector\", \"DBM+SVD\", \"DBM+PCA\", \"DBM\", \"DBM+SAE\"]  # Only run DAS\n",
    ")\n",
    "\n",
    "print(f\"\\nDAS training completed for {target_variable}\")\n",
    "print(f\"Models saved to: {token_model_dir}\")\n",
    "\n",
    "# Clear memory\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Linear Parameters for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:18:00.655346Z",
     "iopub.status.busy": "2025-06-28T07:18:00.655141Z",
     "iopub.status.idle": "2025-06-28T07:18:00.658530Z",
     "shell.execute_reply": "2025-06-28T07:18:00.658269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear parameters saved to: ioi_submission/ioi_linear_params.json\n",
      "{\n",
      "  \"gemma\": {\n",
      "    \"bias\": 0.04835902899503708,\n",
      "    \"token_coeff\": 0.767971899360421,\n",
      "    \"position_coeff\": 2.004627879709005\n",
      "  },\n",
      "  \"model_class\": \"Gemma2ForCausalLM\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Save the linear parameters we learned\n",
    "linear_params_file = os.path.join(model_dir, \"ioi_linear_params.json\")\n",
    "\n",
    "params_to_save = {\n",
    "    model_name: linear_params,\n",
    "    \"model_class\": pipeline.model.__class__.__name__\n",
    "}\n",
    "\n",
    "with open(linear_params_file, 'w') as f:\n",
    "    json.dump(params_to_save, f, indent=2)\n",
    "\n",
    "print(f\"Linear parameters saved to: {linear_params_file}\")\n",
    "print(json.dumps(params_to_save, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Verify Saved Models\n",
    "\n",
    "Let's verify that the models were saved correctly by listing the saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:18:00.660771Z",
     "iopub.status.busy": "2025-06-28T07:18:00.660574Z",
     "iopub.status.idle": "2025-06-28T07:18:00.664303Z",
     "shell.execute_reply": "2025-06-28T07:18:00.664059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved model structure:\n",
      "\n",
      "ioi_submission/\n",
      "ioi_submission/\n",
      "  ioi_linear_params.json\n",
      "  ioi_task_Gemma2ForCausalLM_output_position/\n",
      "    DAS_Gemma2ForCausalLM_output_position/\n",
      "      AttentionHead(Layer-7,Head-6,Token-all)_featurizer\n",
      "      AttentionHead(Layer-7,Head-6,Token-all)_indices\n",
      "      AttentionHead(Layer-7,Head-6,Token-all)_inverse_featurizer\n",
      "      AttentionHead(Layer-8,Head-1,Token-all)_featurizer\n",
      "      AttentionHead(Layer-8,Head-1,Token-all)_indices\n",
      "      AttentionHead(Layer-8,Head-1,Token-all)_inverse_featurizer\n",
      "  ioi_task_Gemma2ForCausalLM_output_token/\n",
      "    DAS_Gemma2ForCausalLM_output_token/\n",
      "      AttentionHead(Layer-7,Head-6,Token-all)_featurizer\n",
      "      AttentionHead(Layer-7,Head-6,Token-all)_indices\n",
      "      AttentionHead(Layer-7,Head-6,Token-all)_inverse_featurizer\n",
      "      AttentionHead(Layer-8,Head-1,Token-all)_featurizer\n",
      "      AttentionHead(Layer-8,Head-1,Token-all)_indices\n",
      "      AttentionHead(Layer-8,Head-1,Token-all)_inverse_featurizer\n",
      "\n",
      "✓ Submission ready!\n",
      "\n",
      "Your submission folder 'ioi_submission' contains:\n",
      "- Trained DAS featurizers for both output_position and output_token\n",
      "- Linear parameters used for the causal model\n",
      "- All necessary files for evaluation\n"
     ]
    }
   ],
   "source": [
    "# List saved model files\n",
    "print(\"\\nSaved model structure:\")\n",
    "print(f\"\\n{model_dir}/\")\n",
    "\n",
    "for root, dirs, files in os.walk(model_dir):\n",
    "    level = root.replace(model_dir, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in sorted(files)[:10]:  # Show first 10 files\n",
    "        print(f\"{subindent}{file}\")\n",
    "    if len(files) > 10:\n",
    "        print(f\"{subindent}... and {len(files) - 10} more files\")\n",
    "\n",
    "print(\"\\n✓ Submission ready!\")\n",
    "print(f\"\\nYour submission folder '{model_dir}' contains:\")\n",
    "print(\"- Trained DAS featurizers for both output_position and output_token\")\n",
    "print(\"- Linear parameters used for the causal model\")\n",
    "print(\"- All necessary files for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Load Trained Models and Run Inference\n",
    "\n",
    "This section demonstrates how to load previously trained featurizers and use them for inference on test data. This is useful for:\n",
    "\n",
    "1. **Testing trained models**: Verify that saved models work correctly\n",
    "2. **Running interventions**: Use the trained featurizers to perform causal interventions on attention heads\n",
    "3. **Evaluation**: Test model performance on held-out test data\n",
    "\n",
    "The process involves:\n",
    "- Loading the trained DAS featurizers from disk\n",
    "- Running interventions on test datasets for both output_position and output_token variables\n",
    "- Collecting results for analysis\n",
    "\n",
    "This is exactly what the evaluation system will do with your submitted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:18:00.666484Z",
     "iopub.status.busy": "2025-06-28T07:18:00.666107Z",
     "iopub.status.idle": "2025-06-28T07:19:17.358735Z",
     "shell.execute_reply": "2025-06-28T07:19:17.357948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained models and running inference...\n",
      "============================================================\n",
      "\n",
      "Testing DAS method for output_position...\n",
      "Loading featurizers from: ioi_submission/ioi_task_Gemma2ForCausalLM_output_position/DAS_Gemma2ForCausalLM_output_position\n",
      "Running interventions for s1_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DAS method for output_token...\n",
      "Loading featurizers from: ioi_submission/ioi_task_Gemma2ForCausalLM_output_token/DAS_Gemma2ForCausalLM_output_token\n",
      "Running interventions for s1_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s2_io_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_test with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:05<00:00,  5.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running interventions for s1_ioi_flip_s2_ioi_flip_testprivate with model units [[AtomicModelUnit(id='AttentionHead(Layer-7,Head-6,Token-all)'), AtomicModelUnit(id='AttentionHead(Layer-8,Head-1,Token-all)')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing batches: 100%|██████████| 1/1 [00:06<00:00,  6.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Inference completed!\n",
      "Results saved to: ioi_submission_results_loaded\n"
     ]
    }
   ],
   "source": [
    "# Load saved models and run inference\n",
    "# This demonstrates how to load previously trained featurizers and run interventions\n",
    "\n",
    "from CausalAbstraction.experiments.attention_head_experiment import PatchAttentionHeads\n",
    "\n",
    "print(\"Loading trained models and running inference...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Directory for saving inference results\n",
    "inference_results_dir = results_dir + \"_loaded\"\n",
    "if not os.path.exists(inference_results_dir):\n",
    "    os.makedirs(inference_results_dir)\n",
    "\n",
    "# Test both target variables\n",
    "target_variables_to_test = [\"output_position\", \"output_token\"]\n",
    "\n",
    "for target_variable in target_variables_to_test:\n",
    "    print(f\"\\nTesting DAS method for {target_variable}...\")\n",
    "    \n",
    "    config[\"n_features\"] = 32\n",
    "    \n",
    "    # Create experiment with same configuration\n",
    "    config[\"method_name\"] = \"DAS\"\n",
    "    experiment = PatchAttentionHeads(\n",
    "        pipeline=pipeline,\n",
    "        causal_model=causal_model,\n",
    "        layer_head_list=heads_list,\n",
    "        token_positions=token_positions,\n",
    "        checker=lambda logits, params: checker(logits, params, pipeline),\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Load the trained featurizers\n",
    "    method_model_dir = os.path.join(\n",
    "        model_dir, \n",
    "        f\"ioi_task_{pipeline.model.__class__.__name__}_{target_variable}\",\n",
    "        f\"DAS_{pipeline.model.__class__.__name__}_{target_variable}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading featurizers from: {method_model_dir}\")\n",
    "    experiment.load_featurizers(method_model_dir)\n",
    "    \n",
    "    # Run interventions on test data\n",
    "    raw_results = experiment.perform_interventions(\n",
    "        test_data, \n",
    "        verbose=True, \n",
    "        target_variables_list=[[target_variable]], \n",
    "        save_dir=inference_results_dir\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    del experiment, raw_results\n",
    "    clear_memory()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Inference completed!\")\n",
    "print(f\"Results saved to: {inference_results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "05978e94a7dd4fb4af9bdf58452c0c9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15d5b5162db742c0a853e33d87c089ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "18db20874ee14dd29a56568063a5c821": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1eecc0f7f5034b94ab89e2619126d204": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "46ac8b840882490bb879e19bb4f94851": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "57dcc5a92f57435a99117ae836418ce3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_18db20874ee14dd29a56568063a5c821",
       "placeholder": "​",
       "style": "IPY_MODEL_15d5b5162db742c0a853e33d87c089ea",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "58be7f1063f54e8a94e85745a791904a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1eecc0f7f5034b94ab89e2619126d204",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_46ac8b840882490bb879e19bb4f94851",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "9099dfd30d094f648ac3c2bd924a7cee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "97191feec80f4bb2a4777008d1affb92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_57dcc5a92f57435a99117ae836418ce3",
        "IPY_MODEL_58be7f1063f54e8a94e85745a791904a",
        "IPY_MODEL_e1aec2ca577c40b58e83fcb4212ea0c4"
       ],
       "layout": "IPY_MODEL_b13ceeef18f1467d9f305ea62362f743",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b13ceeef18f1467d9f305ea62362f743": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1aec2ca577c40b58e83fcb4212ea0c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_05978e94a7dd4fb4af9bdf58452c0c9d",
       "placeholder": "​",
       "style": "IPY_MODEL_9099dfd30d094f648ac3c2bd924a7cee",
       "tabbable": null,
       "tooltip": null,
       "value": " 3/3 [00:00&lt;00:00, 76.88it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
