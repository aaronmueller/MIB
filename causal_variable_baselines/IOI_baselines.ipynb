{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ..CausalAbstraction.tasks.IOI_task.ioi_task import get_task, get_token_positions\n",
    "from ..CausalAbstraction.experiments.aggregate_experiments import ioi_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = get_task(hf=True, size=None)\n",
    "print(\"Raw input:\")\n",
    "print(task.raw_all_data[\"input\"][0])\n",
    "task.display_counterfactual_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ..CausalAbstraction.pipeline import LMPipeline\n",
    "import torch\n",
    "from transformers import GPT2Config\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def checker (pred, expected):\n",
    "    return expected in pred\n",
    "\n",
    "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"google/gemma-2-2b\"\n",
    "model_name = 'openai-community/gpt2'\n",
    "if \"gpt2\" in model_name:\n",
    "    config = GPT2Config.from_pretrained(model_name)\n",
    "    config._attn_implementation = \"eager\"\n",
    "pipeline = LMPipeline(model_name, max_new_tokens=1, device=device, dtype=torch.float32, max_length=32, logit_labels=True,position_ids=True, config=config)\n",
    "pipeline.tokenizer.padding_side = \"left\"\n",
    "batch_size = 1024 * 2\n",
    "print(\"DEVICE:\", pipeline.model.device)\n",
    "print(task.raw_all_data)\n",
    "print(\"INPUT:\", task.raw_all_data[\"input\"][0])\n",
    "print(\"LABEL:\", task.raw_all_data[\"label\"][0])\n",
    "print(\"PREDICTION:\", pipeline.dump(pipeline.generate(task.raw_all_data[\"input\"][0])))\n",
    "\n",
    "task.filter(pipeline, checker, verbose=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_positions = get_token_positions(pipeline, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.output_dumper = lambda x: x\n",
    "pipeline.return_scores = True\n",
    "\n",
    "def log_diff(logits, params):\n",
    "    \"\"\"\n",
    "    Compute the difference in logit scores between two tokens.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor containing logit scores for tokens\n",
    "        params: Dictionary containing 'name_A', 'name_B', and 'output_token'\n",
    "    \"\"\"\n",
    "    # Extract names from params\n",
    "    name_A = params[\"name_A\"]\n",
    "    name_B = params[\"name_B\"]\n",
    "    name_C = params[\"name_C\"]\n",
    "    \n",
    "    if not isinstance(name_A, list):\n",
    "        name_A = [name_A]\n",
    "    if not isinstance(name_B, list):\n",
    "        name_B = [name_B]\n",
    "    if not isinstance(name_C, list):\n",
    "        name_C = [name_C]\n",
    "    # print(name_A, name_B, name_C)\n",
    "\n",
    "    token_id_A = [pipeline.tokenizer.encode(A, add_special_tokens=False)[0] for A in name_A]\n",
    "    token_id_B = [pipeline.tokenizer.encode(B, add_special_tokens=False)[0] for B in name_B]\n",
    "    token_id_C = [pipeline.tokenizer.encode(C, add_special_tokens=False)[0] for C in name_C]\n",
    "\n",
    "    token_id_IO, token_id_S = [], []\n",
    "    for i in range(len(token_id_A)):\n",
    "        if token_id_A[i] == token_id_C[i]:\n",
    "            token_id_S.append(token_id_A[i])\n",
    "            token_id_IO.append(token_id_B[i])\n",
    "        elif token_id_B[i] == token_id_C[i]:\n",
    "            token_id_S.append(token_id_B[i])\n",
    "            token_id_IO.append(token_id_A[i])\n",
    "    # print(token_id_S)\n",
    "    \n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    # print(logits.shape)\n",
    "    # print(len(token_id_S), len(token_id_IO))\n",
    "    # Get the logit scores for both tokens\n",
    "    if len(logits.shape) == 3:\n",
    "        logits = logits.squeeze(1)\n",
    "    if len(logits.shape) == 2:\n",
    "        # Create batch indices\n",
    "        batch_indices = torch.arange(logits.shape[0])\n",
    "        \n",
    "        # Extract specific logits using batch indices\n",
    "        logit_S = logits[batch_indices, token_id_S]\n",
    "        # print(\"2\", logit_S)\n",
    "        logit_IO = logits[batch_indices, token_id_IO]\n",
    "    elif len(logits.shape) == 1:\n",
    "        logit_S = logits[token_id_S[0]]\n",
    "        logit_IO = logits[token_id_IO[0]]\n",
    "\n",
    "    # print(\"S\", logit_S)\n",
    "    # print(\"IO\", logit_IO)\n",
    "    \n",
    "    return logit_IO - logit_S\n",
    "\n",
    "def checker(logits, params):\n",
    "    \"\"\"\n",
    "    Compute the squared error between the actual logit difference and the target logit difference.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor containing logit scores for tokens\n",
    "        params: Dictionary containing 'name_A', 'name_B', 'output_token', and 'logit_diff'\n",
    "    \n",
    "    Returns:\n",
    "        Squared error between the computed logit difference and the target logit difference\n",
    "    \"\"\"\n",
    "    # Extract names and target values from params\n",
    "    if isinstance(logits, list):\n",
    "        logits = logits[0]\n",
    "\n",
    "    target_diff = params[\"logit_diff\"]\n",
    "    actual_diff = log_diff(logits, params)\n",
    "    if isinstance(target_diff, torch.Tensor):\n",
    "        target_diff = target_diff.to(actual_diff.device).to(actual_diff.dtype)\n",
    "        # print(target_diff.shape)\n",
    "        # print(actual_diff.shape)\n",
    "        #make sure the target_diff requires gradient\n",
    "    # Compute the squared error\n",
    "\n",
    "    squared_error = (actual_diff - target_diff) ** 2\n",
    "    \n",
    "    return squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "new_examples, new_raw_examples = [], []\n",
    "for raw_example, example in zip(task.raw_counterfactual_datasets[\"s1_io_flip_train\"], task.counterfactual_datasets[\"s1_io_flip_train\"]):\n",
    "    new_example = example.copy()\n",
    "    new_raw_example = raw_example.copy()\n",
    "\n",
    "    new_example[\"counterfactual_inputs\"] = [new_example[\"input\"].copy()]\n",
    "    new_raw_example[\"counterfactual_inputs\"] = [new_raw_example[\"input\"]]\n",
    "\n",
    "    new_examples.append(new_example)\n",
    "    new_raw_examples.append(new_raw_example)\n",
    "\n",
    "task.counterfactual_datasets[\"same\"] = new_examples\n",
    "task.raw_counterfactual_datasets[\"same\"] = new_raw_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffs = []\n",
    "# losses = []\n",
    "# for raw_example, example in zip(task.raw_counterfactual_datasets[\"same\"], task.counterfactual_datasets[\"same\"]):\n",
    "#     logits = pipeline.generate(raw_example[\"input\"])[0]\n",
    "#     params = task.causal_model.run_forward(example[\"input\"])\n",
    "#     diff = log_diff(logits, params)\n",
    "#     loss = checker(logits, params)\n",
    "#     losses.append(loss)\n",
    "#     diffs.append(diff)\n",
    "# #take the average\n",
    "# diff = sum(diffs) / len(diffs)\n",
    "# loss = sum(losses) / len(losses)\n",
    "# print(\"AVERAGE DIFF:\", diff)\n",
    "# print(\"AVERAGE LOSS:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ..CausalAbstraction.experiments.LM_experiments import PatchIOIHeads\n",
    "\n",
    "\n",
    "\n",
    "data_to_X = {\"same\":{\"position\":1, \"token\":1}, \n",
    "             \"s1_io_flip_train\":{\"position\": -1,\"token\":1},\n",
    "             \"s2_io_flip_train\":{\"position\":-1, \"token\":-1},\n",
    "             \"s1_ioi_flip_s2_ioi_flip_train\":{\"position\":1,\n",
    "                                              \"token\":-1}}\n",
    "X, y, = [], []\n",
    "total_loss = 0\n",
    "\n",
    "for counterfactual in data_to_X:\n",
    "    experiment = PatchIOIHeads(pipeline, task, list(range(0, 1)), None, token_positions, checker, config={\"evaluation_batch_size\": batch_size, \"output_scores\":True})\n",
    "    raw_results = experiment.perform_interventions([counterfactual], verbose=False)\n",
    "    raw_outputs = None\n",
    "    losses, labels, counterfactual_y = [],[],[]  # Collect y values for the current counterfactual\n",
    "    for v in raw_results[\"dataset\"][counterfactual].values():\n",
    "        for v2 in v.values():\n",
    "            raw_outputs = v2[\"raw_outputs\"][0]\n",
    "    for raw_logits, input in zip(raw_outputs, task.counterfactual_datasets[counterfactual]):\n",
    "        actual_diff = log_diff(raw_logits, task.causal_model.run_forward(input[\"input\"]))\n",
    "        high_level_output = task.causal_model.run_interchange(input[\"input\"], {\"output_token\":input[\"counterfactual_inputs\"][0], \"output_position\":input[\"counterfactual_inputs\"][0]})\n",
    "        loss = checker(raw_logits, high_level_output)\n",
    "        label = high_level_output[\"logit_diff\"]\n",
    "        # print(actual_diff)\n",
    "        # print(label)\n",
    "        # print(loss)\n",
    "\n",
    "        y.append(actual_diff)\n",
    "        counterfactual_y.append(actual_diff)  # Append to the counterfactual-specific list\n",
    "        X.append((data_to_X[counterfactual][\"position\"], data_to_X[counterfactual][\"token\"]))\n",
    "        losses.append(loss)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Compute and print the average y for the current counterfactual\n",
    "    avg_y = sum(counterfactual_y) / len(counterfactual_y) if counterfactual_y else 0\n",
    "    print(f\"Average y for counterfactual '{counterfactual}': {avg_y}\")\n",
    "    print(f\"Average label for counterfactual '{counterfactual}': {sum(labels) / len(labels)}\")    \n",
    "    print(f\"Average loss for counterfactual '{counterfactual}': {sum(losses) / len(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING BATCHES\n",
    "\n",
    "# from experiments.LM_experiments import PatchIOIHeads\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# data_to_X = {\"same\":{\"position\":1, \"token\":1}, \n",
    "#              \"s1_io_flip_train\":{\"position\": -1,\"token\":1},\n",
    "#              \"s2_io_flip_train\":{\"position\":-1, \"token\":-1},\n",
    "#              \"s1_ioi_flip_s2_ioi_flip_train\":{\"position\":1,\n",
    "#                                               \"token\":-1}}\n",
    "# X, y, = [], []\n",
    "# total_loss = 0\n",
    "\n",
    "# for counterfactual in data_to_X:\n",
    "#     experiment = PatchIOIHeads(pipeline, task, list(range(0, 1)), token_positions, checker, config={\"evaluation_batch_size\": batch_size, \"output_scores\":True})\n",
    "#     raw_results = experiment.perform_interventions([counterfactual], verbose=False)\n",
    "#     raw_outputs = None\n",
    "#     losses, labels, counterfactual_y = [],[],[]  # Collect y values for the current counterfactual\n",
    "#     for v in raw_results[\"dataset\"][counterfactual].values():\n",
    "#         for v2 in v.values():\n",
    "#             raw_outputs = v2[\"raw_outputs\"][0]\n",
    "#     for raw_logits, input in zip(raw_outputs, task.counterfactual_datasets[counterfactual]):\n",
    "#         print(input)\n",
    "#         print(task.counterfactual_datasets[counterfactual])\n",
    "#         actual_diff = log_diff(raw_logits, task.causal_model.run_forward(input[\"input\"]))\n",
    "#         high_level_output = task.causal_model.run_interchange(input[\"input\"], {\"output_token\":input[\"counterfactual_inputs\"][0], \"output_position\":input[\"counterfactual_inputs\"][0]})\n",
    "#         loss = checker(raw_logits, high_level_output)\n",
    "#         label = high_level_output[\"logit_diff\"]\n",
    "#         break\n",
    "#         # print(input)\n",
    "#         # print(actual_diff)\n",
    "#         # print(label)\n",
    "#         # print(loss)\n",
    "#         # print(raw_logits)\n",
    "#         # print()\n",
    "#     dataloader = DataLoader(\n",
    "#         task.label_counterfactual_data(task.counterfactual_datasets[counterfactual], [\"output_token\", \"output_position\"]),\n",
    "#         batch_size=8,\n",
    "#     )\n",
    "#     for i, batch in enumerate(dataloader):\n",
    "#         raw_logits = raw_outputs[i*8:i*8+8]\n",
    "#         # print()\n",
    "#         # print()\n",
    "#         # print(batch)\n",
    "#         actual_diff = log_diff(raw_logits, batch[\"label\"])\n",
    "#         loss = checker(raw_logits, batch[\"label\"])\n",
    "#         label = batch[\"label\"][\"logit_diff\"]\n",
    "#         # print(actual_diff)\n",
    "#         # print(label)\n",
    "#         # print(loss)\n",
    "#         # print(raw_logits)\n",
    "#         # print(awefa)\n",
    "\n",
    "#         y += actual_diff\n",
    "#         counterfactual_y += actual_diff  # Append to the counterfactual-specific list\n",
    "#         losses += loss\n",
    "#         labels += label\n",
    "    \n",
    "#     # Compute and print the average y for the current counterfactual\n",
    "#     avg_y = sum(counterfactual_y) / len(counterfactual_y) if counterfactual_y else 0\n",
    "#     print(f\"Average y for counterfactual '{counterfactual}': {avg_y}\")\n",
    "#     print(f\"Average label for counterfactual '{counterfactual}': {sum(labels) / len(labels)}\")    \n",
    "#     print(f\"Average loss for counterfactual '{counterfactual}': {sum(losses) / len(losses)}\")\n",
    "#     total_loss += sum(losses) / len(losses)\n",
    "# print(\"TOTAL LOSS:\", total_loss/4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a linear model to the data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)\n",
    "model.fit(X, y) \n",
    "#the loss function is the mean squared error\n",
    "loss = model.score(X, y)\n",
    "# Print the coefficients\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logit_diff(name_A, name_B, name_C, output_token, output_position):\n",
    "    token_signal = None \n",
    "    if (name_C == name_A and output_token == name_B) or (name_C == name_B and output_token == name_A):\n",
    "        token_signal = 1\n",
    "    elif (name_C == name_A and output_token == name_A) or (name_C == name_B and output_token == name_B):\n",
    "        token_signal = -1\n",
    "\n",
    "    position_signal = None \n",
    "    if (name_C == name_A and output_position == 1) or (name_C == name_B and output_position == 0):\n",
    "        position_signal = 1\n",
    "    elif (name_C == name_A and output_position == 0) or (name_C == name_B and output_position == 1):\n",
    "        position_signal = -1\n",
    "\n",
    "    return model.intercept_ + model.coef_[1]* token_signal + model.coef_[0]* position_signal\n",
    "\n",
    "task.causal_model.mechanisms[\"logit_diff\"] = get_logit_diff\n",
    "# custom_loss = lambda logits, params: checker(logits, params).sum()\n",
    "def custom_loss(logits, params):\n",
    "    #average loss\n",
    "    return checker(logits, params).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_X = {\"same\":{\"position\":1, \"token\":1}, \n",
    "             \"s1_io_flip_train\":{\"position\": -1,\"token\":1},\n",
    "             \"s2_io_flip_train\":{\"position\":-1, \"token\":-1},\n",
    "             \"s1_ioi_flip_s2_ioi_flip_train\":{\"position\":1,\n",
    "                                              \"token\":-1}}\n",
    "X, y, = [], []\n",
    "total_loss = 0\n",
    "\n",
    "for counterfactual in data_to_X:\n",
    "    experiment = PatchIOIHeads(pipeline, task, list(range(0, 1)), None, token_positions, checker, config={\"evaluation_batch_size\": batch_size, \"output_scores\":True})\n",
    "    raw_results = experiment.perform_interventions([counterfactual], verbose=False)\n",
    "    raw_outputs = None\n",
    "    losses, labels, counterfactual_y = [],[],[]  # Collect y values for the current counterfactual\n",
    "    for v in raw_results[\"dataset\"][counterfactual].values():\n",
    "        for v2 in v.values():\n",
    "            raw_outputs = v2[\"raw_outputs\"][0]\n",
    "    for raw_logits, input in zip(raw_outputs, task.counterfactual_datasets[counterfactual]):\n",
    "        actual_diff = log_diff(raw_logits, task.causal_model.run_forward(input[\"input\"]))\n",
    "        high_level_output = task.causal_model.run_interchange(input[\"input\"], {\"output_token\":input[\"counterfactual_inputs\"][0], \"output_position\":input[\"counterfactual_inputs\"][0]})\n",
    "        loss = checker(raw_logits, high_level_output)\n",
    "        label = high_level_output[\"logit_diff\"]\n",
    "        # print(actual_diff)\n",
    "        # print(high_level_output[\"logit_diff\"])\n",
    "        # print(loss)\n",
    "\n",
    "        y.append(actual_diff)\n",
    "        counterfactual_y.append(actual_diff)  # Append to the counterfactual-specific list\n",
    "        X.append((data_to_X[counterfactual][\"position\"], data_to_X[counterfactual][\"token\"]))\n",
    "        losses.append(loss)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Compute and print the average y for the current counterfactual\n",
    "    avg_y = sum(counterfactual_y) / len(counterfactual_y) if counterfactual_y else 0\n",
    "    print(f\"Average y for counterfactual '{counterfactual}': {avg_y}\")\n",
    "    print(f\"Average label for counterfactual '{counterfactual}': {sum(labels) / len(labels)}\")    \n",
    "    print(f\"Average loss for counterfactual '{counterfactual}': {sum(losses) / len(losses)}\")\n",
    "    total_loss += sum(losses) / len(losses)\n",
    "print(\"TOTAL LOSS:\", total_loss/4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "start = 0\n",
    "end = pipeline.get_num_layers()\n",
    "config={\"evaluation_batch_size\": batch_size,\"batch_size\":256, \"training_epoch\":2, \"n_features\":32, \"regularization_coefficient\":0.0, \"output_scores\":True, \"shuffle\":True, \"temperature_schedule\":(1.0, 0.01), \"init_lr\":1.0}\n",
    "counterfactuals = [\"s1_io_flip\", \"s2_io_flip\", \"s1_ioi_flip_s2_ioi_flip\"]\n",
    "train_data = [counterfactual + \"_train\" for counterfactual in counterfactuals]\n",
    "test_data = [counterfactual + \"_test\" for counterfactual in counterfactuals]\n",
    "test_data += [counterfactual + \"_testprivate\" for counterfactual in counterfactuals]\n",
    "verbose = True\n",
    "results_dir = \"ioi_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "for m in tqdm([1,2,3,4]):\n",
    "    for layer_heads_list in tqdm(list(itertools.combinations([(7, 3), (7, 9), (8, 6), (8, 10)], m)), desc=\"Layer heads combinations\"):\n",
    "        print(\"Running IOI over Layer heads list:\", layer_heads_list)\n",
    "        results_dir = f\"ioi_results_search_{str(layer_heads_list)}\"\n",
    "        ioi_baselines(pipeline=pipeline, task=task, token_positions=token_positions, train_data=train_data, test_data=test_data, config=config, target_variables=[\"output_token\"], checker=checker, custom_loss=custom_loss, start=start, end=end, verbose=verbose, results_dir=results_dir, heads_list=layer_heads_list, skip=[\"DAS\", \"DBM+PCA\", \"DBM\"])\n",
    "        ioi_baselines(pipeline=pipeline, task=task, token_positions=token_positions, train_data=train_data, test_data=test_data, config=config, target_variables=[\"output_position\"], checker=checker, custom_loss=custom_loss, start=start, end=end, verbose=verbose, results_dir=results_dir, heads_list=layer_heads_list, skip=[\"DAS\", \"DBM+PCA\", \"DBM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_baselines(pipeline=pipeline, task=task, token_positions=token_positions, train_data=train_data, test_data=test_data, config=config, target_variables=[\"output_position\"], checker=checker, custom_loss=custom_loss, start=start, end=end, verbose=verbose, results_dir=results_dir, skip=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_baselines(pipeline=pipeline, task=task, token_positions=token_positions, train_data=train_data, test_data=test_data, config=config, target_variables=[\"output_token\"], checker=checker, custom_loss=custom_loss, start=start, end=end, verbose=verbose, results_dir=results_dir, skip=[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
